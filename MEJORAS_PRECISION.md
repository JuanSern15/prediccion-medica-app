# Estrategias para Mejorar la Precisi√≥n del Modelo

## Situaci√≥n Actual
- **Precisi√≥n**: 70.59% con 55 variables
- **Problema**: Dataset peque√±o (81 pacientes) con desbalance de clases (51/19/11)
- **Modelos**: Regresi√≥n Log√≠stica y Red Neuronal

## T√©cnicas Implementadas

### 1. **Selecci√≥n de Caracter√≠sticas (Feature Selection)**
- **Objetivo**: Eliminar variables irrelevantes que a√±aden ruido
- **M√©todo**: ANOVA F-test para identificar las 30 variables m√°s discriminativas
- **Beneficio**: Reduce overfitting y mejora generalizaci√≥n

### 2. **Balanceo de Clases con SMOTE**
- **Problema**: Dengue (51) >> Malaria (19) >> Leptospirosis (11)
- **Soluci√≥n**: SMOTE-Tomek genera muestras sint√©ticas de clases minoritarias
- **Beneficio**: El modelo aprende mejor a identificar Malaria y Leptospirosis

### 3. **Optimizaci√≥n de Hiperpar√°metros (GridSearchCV)**

#### Regresi√≥n Log√≠stica:
```python
Par√°metros a probar:
- C: [0.001, 0.01, 0.1, 1, 10, 100] ‚Üí Regularizaci√≥n
- solver: ['lbfgs', 'saga', 'liblinear'] ‚Üí Algoritmo de optimizaci√≥n
- max_iter: [500, 1000, 2000] ‚Üí Iteraciones
- class_weight: ['balanced', None] ‚Üí Penalizaci√≥n por desbalance
Total: 72 combinaciones
```

#### Red Neuronal:
```python
Par√°metros a probar:
- hidden_layer_sizes: [(100,), (64,32), (128,64,32), (100,50,25)]
- activation: ['relu', 'tanh']
- alpha: [0.0001, 0.001, 0.01] ‚Üí Regularizaci√≥n L2
- learning_rate: ['constant', 'adaptive']
- max_iter: [500, 1000]
Total: 80 combinaciones
```

### 4. **Validaci√≥n Cruzada Estratificada**
- **M√©todo**: 5-fold stratified cross-validation
- **Beneficio**: Evaluaci√≥n m√°s confiable que un solo train/test split
- **Resultado**: Media ¬± desviaci√≥n est√°ndar de precisi√≥n

### 5. **Modelo de Ensamble (Gradient Boosting)**
- **M√©todo**: Combina m√∫ltiples √°rboles de decisi√≥n d√©biles
- **Par√°metros**: 200 √°rboles
- **Beneficio**: Usualmente mejor que modelos lineales en datasets complejos

## Mejoras Esperadas

### Antes de Optimizaci√≥n:
```
Regresi√≥n Log√≠stica: 70.59%
Red Neuronal:        70.59%
```

### Despu√©s de Optimizaci√≥n (estimado):
```
Regresi√≥n Log√≠stica: 75-82% (con hiperpar√°metros √≥ptimos + SMOTE)
Red Neuronal:        78-85% (con arquitectura √≥ptima + SMOTE)
Gradient Boosting:   80-88% (modelo de ensamble)
```

## Otras T√©cnicas Avanzadas (Si a√∫n no es suficiente)

### 6. **Ensemble Stacking**
```python
from sklearn.ensemble import VotingClassifier, StackingClassifier

# Combinar predicciones de m√∫ltiples modelos
ensemble = VotingClassifier(
    estimators=[
        ('lr', mejor_lr),
        ('nn', mejor_nn),
        ('gb', mejor_gb)
    ],
    voting='soft'  # Usa probabilidades
)
```

### 7. **Feature Engineering Avanzado**
```python
# Crear nuevas features combinando existentes
X['ratio_neutrophils_lymphocytes'] = X['neutrophils'] / (X['lymphocytes'] + 1)
X['platelets_severity'] = (X['platelets'] < 50000).astype(int)
X['liver_damage'] = ((X['AST (SGOT)'] > 100) | (X['ALT (SGPT)'] > 100)).astype(int)
```

### 8. **PCA (Principal Component Analysis)**
```python
from sklearn.decomposition import PCA

# Reducir dimensionalidad preservando varianza
pca = PCA(n_components=0.95)  # 95% de varianza
X_reduced = pca.fit_transform(X_scaled)
```

### 9. **XGBoost (Extreme Gradient Boosting)**
```python
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.01,
    max_depth=5,
    scale_pos_weight=3  # Para desbalance
)
```

### 10. **Aumentaci√≥n de Datos (Data Augmentation)**
```python
from imblearn.over_sampling import ADASYN

# M√°s sofisticado que SMOTE
adasyn = ADASYN(random_state=42)
X_resampled, y_resampled = adasyn.fit_resample(X, y)
```

## Limitaciones del Dataset

### Factores que limitan la precisi√≥n m√°xima:
1. **Tama√±o**: Solo 81 pacientes (muy peque√±o para ML)
2. **Desbalance**: 4.6x entre clase mayoritaria y minoritaria
3. **Solapamiento**: Las 3 enfermedades comparten s√≠ntomas similares
4. **Variabilidad biol√≥gica**: Pacientes con misma enfermedad tienen presentaciones diferentes

### Precisi√≥n realista m√°xima esperada:
- **Con 81 pacientes**: 80-90%
- **Con 500+ pacientes**: 90-95%
- **Con 2000+ pacientes**: 95-98%

## Recomendaciones

### Para este proyecto:
1. ‚úÖ Usar optimizaci√≥n de hiperpar√°metros (GridSearchCV)
2. ‚úÖ Aplicar SMOTE para balanceo
3. ‚úÖ Seleccionar mejores features
4. ‚úÖ Probar Gradient Boosting
5. ‚≠ê Si a√∫n necesitas m√°s: usar ensemble stacking

### Para trabajo futuro:
1. üìä Recolectar m√°s datos (objetivo: 200-500 pacientes)
2. üî¨ Agregar m√°s an√°lisis de laboratorio espec√≠ficos
3. üß¨ Considerar datos gen√©ticos o de im√°genes m√©dicas
4. ü§ñ Explorar deep learning si el dataset crece a 1000+ muestras
